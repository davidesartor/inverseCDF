{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "import random\n",
    "from torch.utils.data import TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings \n",
    "import pandas as pd\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_mono = np.array([-1, 1, -1, -1, 1] + [0] * 23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input(inputs):\n",
    "    return inputs[:, np.where(mask_mono==0)].squeeze(), \\\n",
    "        inputs[:, np.where(mask_mono!=0)].squeeze() * torch.tensor(mask_mono[np.where(mask_mono!=0)][None,:], dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=50):\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "    best_val_acc = 0.\n",
    "    for _ in trange(num_epochs):\n",
    "        model.train()\n",
    "        total = 0\n",
    "        losses_buffer = []\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs_free, inputs_mono = split_input(inputs)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs_free.float(), inputs_mono.float())\n",
    "            loss = criterion(outputs, labels.float())\n",
    "            losses_buffer.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total += labels.size(0)\n",
    "        losses.append(np.mean([el.detach().cpu() for el in losses_buffer]))\n",
    "        \n",
    "        val_acc, val_loss = validate_model(model, val_loader, criterion)\n",
    "        best_val_acc = max(best_val_acc, val_acc)\n",
    "        val_losses.append(val_loss)\n",
    "    \n",
    "    return losses, val_losses, best_val_acc\n",
    "\n",
    "def validate_model(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs_free, inputs_mono = split_input(inputs)\n",
    "            outputs = model(inputs_free.float(), inputs_mono.float())\n",
    "            loss = criterion(outputs, labels.float())\n",
    "            val_loss += [loss.item()]\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (outputs.round() == labels).sum().item()\n",
    "\n",
    "    val_accuracy = correct / total\n",
    "    return val_accuracy, np.mean(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonotonicLinear(nn.Linear):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int, \n",
    "        out_features: int, \n",
    "        bias: bool = True,\n",
    "        device=None, \n",
    "        dtype=None,\n",
    "        pre_activation=nn.Identity(),\n",
    "    ):\n",
    "        super().__init__(in_features, out_features, bias=bias, device=device, dtype=dtype)\n",
    "        self.act = pre_activation\n",
    "        \n",
    "    def forward(self, x):\n",
    "        w_pos = self.weight.clamp(min=0.0)\n",
    "        w_neg = self.weight.clamp(max=0.0)\n",
    "        x_pos = F.linear(self.act(x), w_pos, self.bias)\n",
    "        x_neg = F.linear(self.act(-x), w_neg, self.bias)  \n",
    "        return x_pos + x_neg\n",
    "    \n",
    "class MonoModel(torch.nn.Module):\n",
    "    def __init__(self, input_size_mono, num_layers_mono, num_layers_pre_mono, num_neurons_mono, num_neurons_pre_mono, activation=nn.ReLU()) -> None:\n",
    "        super().__init__()\n",
    "        self.pre_mono = torch.nn.ModuleList([torch.nn.LazyLinear(num_neurons_pre_mono) for _ in range(num_layers_pre_mono)])\n",
    "        self.mono = torch.nn.ModuleList(\n",
    "            [\n",
    "                MonotonicLinear(input_size_mono + num_neurons_pre_mono, num_neurons_mono, pre_activation=nn.Identity()),\n",
    "                *[MonotonicLinear(num_neurons_mono, num_neurons_mono, pre_activation=activation) for _ in range(num_layers_mono)],\n",
    "                MonotonicLinear(num_neurons_mono, 1, pre_activation=activation),\n",
    "            ]\n",
    "        )\n",
    "    def forward(self, x, x_mono):\n",
    "        for layer in self.pre_mono:\n",
    "            x = torch.nn.functional.relu(layer(x))\n",
    "        \n",
    "        x = torch.cat((x, x_mono), dim=-1)\n",
    "        for layer in self.mono:\n",
    "            x = layer(x)\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(SEED, n, lr=1e-3, activation=nn.ReLU()):\n",
    "    torch.manual_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    random.seed(SEED)\n",
    "\n",
    "    df_train = pd.read_csv('data/train_loan.csv',header = None)\n",
    "    df_train = df_train.dropna(axis=0)        \n",
    "    X_train = df_train.to_numpy()[:,:-1]\n",
    "    y_train = df_train.to_numpy()[:,-1:]\n",
    "\n",
    "    df_val = pd.read_csv('data/test_loan.csv',header = None)\n",
    "    df_val = df_val.dropna(axis=0)\n",
    "    X_val = df_val.to_numpy()[:,:-1]\n",
    "    y_val = df_val.to_numpy()[:,-1:]\n",
    "\n",
    "    X_train = torch.tensor(X_train).to(device).float()\n",
    "    X_val = torch.tensor(X_val).to(device).float()\n",
    "    y_train = torch.tensor(y_train).to(device).float()\n",
    "    y_val = torch.tensor(y_val).to(device).float()\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(TensorDataset(X_train, y_train), batch_size=256, shuffle=True, drop_last=True)\n",
    "    val_loader = torch.utils.data.DataLoader(TensorDataset(X_val, y_val), batch_size=256, shuffle=True, drop_last=True)\n",
    "    \n",
    "    model = MonoModel((mask_mono!=0).sum(),3,3, n,n, activation=activation).to(device)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "    losses, val_losses, best_val_acc = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=50)\n",
    "    print(\"SEED\", SEED, \"VAL LOSS\", np.min(val_losses), \"BEST ACC\", best_val_acc)\n",
    "    return losses, val_losses, best_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [08:09<00:00,  9.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED 0 VAL LOSS 0.21636439011479816 BEST ACC 0.6531421076642335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [07:33<00:00,  9.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED 1 VAL LOSS 0.21629490621768646 BEST ACC 0.6540830291970803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [06:30<00:00,  7.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED 2 VAL LOSS 0.21638492973398987 BEST ACC 0.6533559534671532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [06:34<00:00,  7.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED 3 VAL LOSS 0.21633010156398272 BEST ACC 0.6535840556569343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [06:02<00:00,  7.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED 4 VAL LOSS 0.21632703529657238 BEST ACC 0.6540545164233577\n",
      "---------------------------------\n",
      "Mean acc 0.6536439324817518\n",
      "Std acc 0.00037409367623457104\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n = 16\n",
    "activation = nn.ReLU()\n",
    "train_losses, val_losses, val_acc = [],[],[]\n",
    "for seed in range(5):\n",
    "    ltrain, lval, lacc = run(seed, n, activation=activation)\n",
    "    train_losses.append(ltrain)\n",
    "    val_losses.append(lval)\n",
    "    val_acc.append(lacc)\n",
    "\n",
    "print(\"---------------------------------\")\n",
    "print(\"Mean acc\", np.mean(val_acc))\n",
    "print(\"Std acc\", np.std(val_acc))\n",
    "print(\"---------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mono",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
